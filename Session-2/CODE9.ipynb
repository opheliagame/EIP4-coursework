{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIGHTH.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "outputId": "c89644fe-5e3b-4968-8e8c-679ae02c08f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlEUplvoxKAT",
        "colab_type": "code",
        "outputId": "fe71257c-c525-4c77-b17c-28aaf83e1912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "ee6cd95e-e5fb-4a68-a952-9d945fcf70d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd4e3ad99e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train[:10]\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFR0F9j0xVp2",
        "colab_type": "code",
        "outputId": "dc545678-e893-4a5f-96a6-093880e0f6e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr0GwOiy2ujX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HISTORY = {} \n",
        "'''\n",
        "  contains info about each model\n",
        "  key: intuition behind model\n",
        "  trainable_params: no of trainable params\n",
        "  non_trainable_params: no of non trainable params\n",
        "  scores: list of scores for each training session\n",
        "  val_acc, epoch: highest val_acc and epoch number for each training session\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "07558f90-c7a3-4f6a-d690-958296fb8461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1), use_bias=False)) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu', use_bias=False)) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu', use_bias=False)) #22\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))#9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))#7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))#5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))#3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 4, 4, use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1..., use_bias=False)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (1, 1), activation=\"relu\", use_bias=False)`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_105 (Conv2D)          (None, 26, 26, 10)        90        \n",
            "_________________________________________________________________\n",
            "batch_normalization_92 (Batc (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_92 (Dropout)         (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_106 (Conv2D)          (None, 24, 24, 20)        1800      \n",
            "_________________________________________________________________\n",
            "batch_normalization_93 (Batc (None, 24, 24, 20)        80        \n",
            "_________________________________________________________________\n",
            "dropout_93 (Dropout)         (None, 24, 24, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_107 (Conv2D)          (None, 24, 24, 8)         160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 12, 12, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_108 (Conv2D)          (None, 10, 10, 16)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_94 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_94 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_109 (Conv2D)          (None, 8, 8, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_95 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_95 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_110 (Conv2D)          (None, 6, 6, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_96 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_96 (Dropout)         (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_111 (Conv2D)          (None, 4, 4, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_97 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_97 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_112 (Conv2D)          (None, 1, 1, 10)          2560      \n",
            "_________________________________________________________________\n",
            "batch_normalization_98 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_98 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,090\n",
            "Trainable params: 12,882\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4), use_bias=False)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IicGJ4x3Be",
        "colab_type": "code",
        "outputId": "c6b1b412-539b-4c2d-c5a2-3dd2e20c474f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "history = []\n",
        "scores = []\n",
        "for i in range(4):\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "  h = model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "  history.append(h)\n",
        "  score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "  scores.append(score)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.5289 - acc: 0.8521 - val_loss: 0.0941 - val_acc: 0.9814\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.2575 - acc: 0.9224 - val_loss: 0.0698 - val_acc: 0.9858\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.2017 - acc: 0.9400 - val_loss: 0.0435 - val_acc: 0.9898\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.1731 - acc: 0.9458 - val_loss: 0.0400 - val_acc: 0.9905\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.1540 - acc: 0.9483 - val_loss: 0.0325 - val_acc: 0.9914\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1401 - acc: 0.9510 - val_loss: 0.0333 - val_acc: 0.9917\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.1289 - acc: 0.9547 - val_loss: 0.0306 - val_acc: 0.9916\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.1247 - acc: 0.9541 - val_loss: 0.0280 - val_acc: 0.9919\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.1193 - acc: 0.9545 - val_loss: 0.0259 - val_acc: 0.9934\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.1158 - acc: 0.9534 - val_loss: 0.0234 - val_acc: 0.9933\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1135 - acc: 0.9545 - val_loss: 0.0291 - val_acc: 0.9922\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1089 - acc: 0.9554 - val_loss: 0.0219 - val_acc: 0.9933\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.1068 - acc: 0.9562 - val_loss: 0.0230 - val_acc: 0.9937\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1049 - acc: 0.9551 - val_loss: 0.0210 - val_acc: 0.9939\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.1023 - acc: 0.9557 - val_loss: 0.0218 - val_acc: 0.9938\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.1011 - acc: 0.9550 - val_loss: 0.0215 - val_acc: 0.9935\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0977 - acc: 0.9572 - val_loss: 0.0196 - val_acc: 0.9942\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0961 - acc: 0.9576 - val_loss: 0.0207 - val_acc: 0.9933\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0958 - acc: 0.9564 - val_loss: 0.0204 - val_acc: 0.9943\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0929 - acc: 0.9581 - val_loss: 0.0223 - val_acc: 0.9939\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.1151 - acc: 0.9524 - val_loss: 0.0430 - val_acc: 0.9878\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.1077 - acc: 0.9533 - val_loss: 0.0295 - val_acc: 0.9920\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.1010 - acc: 0.9537 - val_loss: 0.0242 - val_acc: 0.9932\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0957 - acc: 0.9564 - val_loss: 0.0243 - val_acc: 0.9937\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0956 - acc: 0.9553 - val_loss: 0.0224 - val_acc: 0.9930\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0920 - acc: 0.9574 - val_loss: 0.0199 - val_acc: 0.9944\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0896 - acc: 0.9577 - val_loss: 0.0202 - val_acc: 0.9947\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0877 - acc: 0.9581 - val_loss: 0.0196 - val_acc: 0.9945\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0870 - acc: 0.9589 - val_loss: 0.0210 - val_acc: 0.9939\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0858 - acc: 0.9570 - val_loss: 0.0208 - val_acc: 0.9944\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0837 - acc: 0.9594 - val_loss: 0.0204 - val_acc: 0.9939\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0828 - acc: 0.9595 - val_loss: 0.0201 - val_acc: 0.9938\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0844 - acc: 0.9582 - val_loss: 0.0194 - val_acc: 0.9953\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0828 - acc: 0.9598 - val_loss: 0.0211 - val_acc: 0.9943\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0806 - acc: 0.9604 - val_loss: 0.0200 - val_acc: 0.9944\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0812 - acc: 0.9593 - val_loss: 0.0213 - val_acc: 0.9940\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0805 - acc: 0.9587 - val_loss: 0.0193 - val_acc: 0.9945\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0795 - acc: 0.9605 - val_loss: 0.0208 - val_acc: 0.9947\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0801 - acc: 0.9605 - val_loss: 0.0211 - val_acc: 0.9942\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0798 - acc: 0.9591 - val_loss: 0.0199 - val_acc: 0.9946\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 14s 238us/step - loss: 0.0961 - acc: 0.9564 - val_loss: 0.0309 - val_acc: 0.9910\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0935 - acc: 0.9563 - val_loss: 0.0285 - val_acc: 0.9923\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0894 - acc: 0.9564 - val_loss: 0.0277 - val_acc: 0.9931\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0871 - acc: 0.9576 - val_loss: 0.0213 - val_acc: 0.9940\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0845 - acc: 0.9586 - val_loss: 0.0192 - val_acc: 0.9949\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0828 - acc: 0.9581 - val_loss: 0.0203 - val_acc: 0.9945\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0832 - acc: 0.9581 - val_loss: 0.0186 - val_acc: 0.9952\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0791 - acc: 0.9607 - val_loss: 0.0190 - val_acc: 0.9947\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0814 - acc: 0.9581 - val_loss: 0.0183 - val_acc: 0.9949\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0802 - acc: 0.9599 - val_loss: 0.0193 - val_acc: 0.9948\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0790 - acc: 0.9599 - val_loss: 0.0175 - val_acc: 0.9951\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0769 - acc: 0.9609 - val_loss: 0.0184 - val_acc: 0.9946\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0772 - acc: 0.9603 - val_loss: 0.0174 - val_acc: 0.9953\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0759 - acc: 0.9612 - val_loss: 0.0170 - val_acc: 0.9960\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0765 - acc: 0.9606 - val_loss: 0.0188 - val_acc: 0.9953\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0768 - acc: 0.9600 - val_loss: 0.0181 - val_acc: 0.9947\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0740 - acc: 0.9608 - val_loss: 0.0182 - val_acc: 0.9953\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0755 - acc: 0.9605 - val_loss: 0.0170 - val_acc: 0.9955\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0757 - acc: 0.9600 - val_loss: 0.0168 - val_acc: 0.9953\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0759 - acc: 0.9605 - val_loss: 0.0195 - val_acc: 0.9948\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0917 - acc: 0.9569 - val_loss: 0.0237 - val_acc: 0.9930\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0875 - acc: 0.9557 - val_loss: 0.0231 - val_acc: 0.9939\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0836 - acc: 0.9585 - val_loss: 0.0202 - val_acc: 0.9946\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0809 - acc: 0.9582 - val_loss: 0.0210 - val_acc: 0.9940\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0801 - acc: 0.9593 - val_loss: 0.0215 - val_acc: 0.9936\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0787 - acc: 0.9590 - val_loss: 0.0216 - val_acc: 0.9943\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0769 - acc: 0.9591 - val_loss: 0.0204 - val_acc: 0.9943\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0760 - acc: 0.9600 - val_loss: 0.0214 - val_acc: 0.9949\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0758 - acc: 0.9612 - val_loss: 0.0180 - val_acc: 0.9947\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0768 - acc: 0.9598 - val_loss: 0.0174 - val_acc: 0.9952\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0749 - acc: 0.9599 - val_loss: 0.0207 - val_acc: 0.9947\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0759 - acc: 0.9602 - val_loss: 0.0186 - val_acc: 0.9948\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0742 - acc: 0.9607 - val_loss: 0.0202 - val_acc: 0.9946\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0739 - acc: 0.9595 - val_loss: 0.0177 - val_acc: 0.9946\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0738 - acc: 0.9611 - val_loss: 0.0169 - val_acc: 0.9950\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0714 - acc: 0.9623 - val_loss: 0.0176 - val_acc: 0.9952\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0719 - acc: 0.9614 - val_loss: 0.0176 - val_acc: 0.9956\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0742 - acc: 0.9599 - val_loss: 0.0178 - val_acc: 0.9948\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0729 - acc: 0.9612 - val_loss: 0.0178 - val_acc: 0.9946\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0723 - acc: 0.9605 - val_loss: 0.0172 - val_acc: 0.9951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTiZSUK-8LtQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc75627b-1b69-4db6-b919-1e5ca360b296"
      },
      "source": [
        "# find out the highest val_acc in the 4 runs of the model\n",
        "max_acc = [(max(history[i].history['val_acc']), i) for i in range(4)]\n",
        "print(max_acc)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0.9943, 0), (0.9953, 1), (0.996, 2), (0.9956, 3)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxlW9ufyQiO",
        "colab_type": "code",
        "outputId": "df42ac9c-2bab-4bed-af80-9ca607c93644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(scores)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.022299894432898145, 0.9939], [0.019910878463128757, 0.9946], [0.019506690664345298, 0.9948], [0.017212786722480996, 0.9951]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpbZZBkA0Iza",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "1677c840-0d54-4d75-81f2-3aeadd370cbf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(history): # to look at individual history\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRNL6/wEnRz8bwTWnpQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngzjWWT1FAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2qDl21ozBnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "eea9687d-052c-4ef6-c72f-9e8c8d8c1b51"
      },
      "source": [
        "# Careful before running cell\n",
        "HISTORY['10 20 8 kernels before max pooling'] = {\n",
        "    'trainable_params': 12882,\n",
        "    'non_trainable_params': 208,\n",
        "    'scores': scores,\n",
        "    'acc': max_acc\n",
        "}\n",
        "print(HISTORY)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'original without bias': {'trainable_params': 16228, 'non_trainable_params': 244, 'scores': [[0.01808474469958569, 0.9954], [0.019497062973319226, 0.9952], [0.01994227597291588, 0.9949], [0.021506337354881406, 0.9952]], 'acc': [(0.9959, 0), (0.9956, 1), (0.9956, 2), (0.9955, 3)]}, '10 20 8 kernels before max pooling': {'trainable_params': 12882, 'non_trainable_params': 208, 'scores': [[0.022299894432898145, 0.9939], [0.019910878463128757, 0.9946], [0.019506690664345298, 0.9948], [0.017212786722480996, 0.9951]], 'acc': [(0.9943, 0), (0.9953, 1), (0.996, 2), (0.9956, 3)]}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rXSVNAhGmrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HISTORY['10 20 8 kernels before max pooling']['log'] = '''\n",
        "Train on 60000 samples, validate on 10000 samples\n",
        "Epoch 1/20\n",
        "\n",
        "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
        "60000/60000 [==============================] - 15s 248us/step - loss: 0.5289 - acc: 0.8521 - val_loss: 0.0941 - val_acc: 0.9814\n",
        "Epoch 2/20\n",
        "\n",
        "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
        "60000/60000 [==============================] - 6s 100us/step - loss: 0.2575 - acc: 0.9224 - val_loss: 0.0698 - val_acc: 0.9858\n",
        "Epoch 3/20\n",
        "\n",
        "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.2017 - acc: 0.9400 - val_loss: 0.0435 - val_acc: 0.9898\n",
        "Epoch 4/20\n",
        "\n",
        "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.1731 - acc: 0.9458 - val_loss: 0.0400 - val_acc: 0.9905\n",
        "Epoch 5/20\n",
        "\n",
        "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.1540 - acc: 0.9483 - val_loss: 0.0325 - val_acc: 0.9914\n",
        "Epoch 6/20\n",
        "\n",
        "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
        "60000/60000 [==============================] - 6s 100us/step - loss: 0.1401 - acc: 0.9510 - val_loss: 0.0333 - val_acc: 0.9917\n",
        "Epoch 7/20\n",
        "\n",
        "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.1289 - acc: 0.9547 - val_loss: 0.0306 - val_acc: 0.9916\n",
        "Epoch 8/20\n",
        "\n",
        "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.1247 - acc: 0.9541 - val_loss: 0.0280 - val_acc: 0.9919\n",
        "Epoch 9/20\n",
        "\n",
        "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.1193 - acc: 0.9545 - val_loss: 0.0259 - val_acc: 0.9934\n",
        "Epoch 10/20\n",
        "\n",
        "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.1158 - acc: 0.9534 - val_loss: 0.0234 - val_acc: 0.9933\n",
        "Epoch 11/20\n",
        "\n",
        "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.1135 - acc: 0.9545 - val_loss: 0.0291 - val_acc: 0.9922\n",
        "Epoch 12/20\n",
        "\n",
        "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.1089 - acc: 0.9554 - val_loss: 0.0219 - val_acc: 0.9933\n",
        "Epoch 13/20\n",
        "\n",
        "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.1068 - acc: 0.9562 - val_loss: 0.0230 - val_acc: 0.9937\n",
        "Epoch 14/20\n",
        "\n",
        "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.1049 - acc: 0.9551 - val_loss: 0.0210 - val_acc: 0.9939\n",
        "Epoch 15/20\n",
        "\n",
        "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.1023 - acc: 0.9557 - val_loss: 0.0218 - val_acc: 0.9938\n",
        "Epoch 16/20\n",
        "\n",
        "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.1011 - acc: 0.9550 - val_loss: 0.0215 - val_acc: 0.9935\n",
        "Epoch 17/20\n",
        "\n",
        "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0977 - acc: 0.9572 - val_loss: 0.0196 - val_acc: 0.9942\n",
        "Epoch 18/20\n",
        "\n",
        "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0961 - acc: 0.9576 - val_loss: 0.0207 - val_acc: 0.9933\n",
        "Epoch 19/20\n",
        "\n",
        "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0958 - acc: 0.9564 - val_loss: 0.0204 - val_acc: 0.9943\n",
        "Epoch 20/20\n",
        "\n",
        "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0929 - acc: 0.9581 - val_loss: 0.0223 - val_acc: 0.9939\n",
        "Train on 60000 samples, validate on 10000 samples\n",
        "Epoch 1/20\n",
        "\n",
        "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
        "60000/60000 [==============================] - 14s 232us/step - loss: 0.1151 - acc: 0.9524 - val_loss: 0.0430 - val_acc: 0.9878\n",
        "Epoch 2/20\n",
        "\n",
        "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.1077 - acc: 0.9533 - val_loss: 0.0295 - val_acc: 0.9920\n",
        "Epoch 3/20\n",
        "\n",
        "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.1010 - acc: 0.9537 - val_loss: 0.0242 - val_acc: 0.9932\n",
        "Epoch 4/20\n",
        "\n",
        "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0957 - acc: 0.9564 - val_loss: 0.0243 - val_acc: 0.9937\n",
        "Epoch 5/20\n",
        "\n",
        "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0956 - acc: 0.9553 - val_loss: 0.0224 - val_acc: 0.9930\n",
        "Epoch 6/20\n",
        "\n",
        "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0920 - acc: 0.9574 - val_loss: 0.0199 - val_acc: 0.9944\n",
        "Epoch 7/20\n",
        "\n",
        "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
        "60000/60000 [==============================] - 6s 100us/step - loss: 0.0896 - acc: 0.9577 - val_loss: 0.0202 - val_acc: 0.9947\n",
        "Epoch 8/20\n",
        "\n",
        "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0877 - acc: 0.9581 - val_loss: 0.0196 - val_acc: 0.9945\n",
        "Epoch 9/20\n",
        "\n",
        "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0870 - acc: 0.9589 - val_loss: 0.0210 - val_acc: 0.9939\n",
        "Epoch 10/20\n",
        "\n",
        "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0858 - acc: 0.9570 - val_loss: 0.0208 - val_acc: 0.9944\n",
        "Epoch 11/20\n",
        "\n",
        "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0837 - acc: 0.9594 - val_loss: 0.0204 - val_acc: 0.9939\n",
        "Epoch 12/20\n",
        "\n",
        "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
        "60000/60000 [==============================] - 6s 99us/step - loss: 0.0828 - acc: 0.9595 - val_loss: 0.0201 - val_acc: 0.9938\n",
        "Epoch 13/20\n",
        "\n",
        "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0844 - acc: 0.9582 - val_loss: 0.0194 - val_acc: 0.9953\n",
        "Epoch 14/20\n",
        "\n",
        "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0828 - acc: 0.9598 - val_loss: 0.0211 - val_acc: 0.9943\n",
        "Epoch 15/20\n",
        "\n",
        "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0806 - acc: 0.9604 - val_loss: 0.0200 - val_acc: 0.9944\n",
        "Epoch 16/20\n",
        "\n",
        "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
        "60000/60000 [==============================] - 6s 105us/step - loss: 0.0812 - acc: 0.9593 - val_loss: 0.0213 - val_acc: 0.9940\n",
        "Epoch 17/20\n",
        "\n",
        "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0805 - acc: 0.9587 - val_loss: 0.0193 - val_acc: 0.9945\n",
        "Epoch 18/20\n",
        "\n",
        "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
        "60000/60000 [==============================] - 6s 100us/step - loss: 0.0795 - acc: 0.9605 - val_loss: 0.0208 - val_acc: 0.9947\n",
        "Epoch 19/20\n",
        "\n",
        "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0801 - acc: 0.9605 - val_loss: 0.0211 - val_acc: 0.9942\n",
        "Epoch 20/20\n",
        "\n",
        "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0798 - acc: 0.9591 - val_loss: 0.0199 - val_acc: 0.9946\n",
        "Train on 60000 samples, validate on 10000 samples\n",
        "Epoch 1/20\n",
        "\n",
        "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
        "60000/60000 [==============================] - 14s 238us/step - loss: 0.0961 - acc: 0.9564 - val_loss: 0.0309 - val_acc: 0.9910\n",
        "Epoch 2/20\n",
        "\n",
        "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0935 - acc: 0.9563 - val_loss: 0.0285 - val_acc: 0.9923\n",
        "Epoch 3/20\n",
        "\n",
        "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0894 - acc: 0.9564 - val_loss: 0.0277 - val_acc: 0.9931\n",
        "Epoch 4/20\n",
        "\n",
        "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0871 - acc: 0.9576 - val_loss: 0.0213 - val_acc: 0.9940\n",
        "Epoch 5/20\n",
        "\n",
        "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
        "60000/60000 [==============================] - 6s 100us/step - loss: 0.0845 - acc: 0.9586 - val_loss: 0.0192 - val_acc: 0.9949\n",
        "Epoch 6/20\n",
        "\n",
        "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0828 - acc: 0.9581 - val_loss: 0.0203 - val_acc: 0.9945\n",
        "Epoch 7/20\n",
        "\n",
        "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0832 - acc: 0.9581 - val_loss: 0.0186 - val_acc: 0.9952\n",
        "Epoch 8/20\n",
        "\n",
        "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0791 - acc: 0.9607 - val_loss: 0.0190 - val_acc: 0.9947\n",
        "Epoch 9/20\n",
        "\n",
        "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0814 - acc: 0.9581 - val_loss: 0.0183 - val_acc: 0.9949\n",
        "Epoch 10/20\n",
        "\n",
        "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0802 - acc: 0.9599 - val_loss: 0.0193 - val_acc: 0.9948\n",
        "Epoch 11/20\n",
        "\n",
        "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
        "60000/60000 [==============================] - 6s 106us/step - loss: 0.0790 - acc: 0.9599 - val_loss: 0.0175 - val_acc: 0.9951\n",
        "Epoch 12/20\n",
        "\n",
        "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0769 - acc: 0.9609 - val_loss: 0.0184 - val_acc: 0.9946\n",
        "Epoch 13/20\n",
        "\n",
        "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
        "60000/60000 [==============================] - 6s 105us/step - loss: 0.0772 - acc: 0.9603 - val_loss: 0.0174 - val_acc: 0.9953\n",
        "Epoch 14/20\n",
        "\n",
        "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0759 - acc: 0.9612 - val_loss: 0.0170 - val_acc: 0.9960\n",
        "Epoch 15/20\n",
        "\n",
        "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0765 - acc: 0.9606 - val_loss: 0.0188 - val_acc: 0.9953\n",
        "Epoch 16/20\n",
        "\n",
        "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0768 - acc: 0.9600 - val_loss: 0.0181 - val_acc: 0.9947\n",
        "Epoch 17/20\n",
        "\n",
        "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0740 - acc: 0.9608 - val_loss: 0.0182 - val_acc: 0.9953\n",
        "Epoch 18/20\n",
        "\n",
        "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0755 - acc: 0.9605 - val_loss: 0.0170 - val_acc: 0.9955\n",
        "Epoch 19/20\n",
        "\n",
        "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0757 - acc: 0.9600 - val_loss: 0.0168 - val_acc: 0.9953\n",
        "Epoch 20/20\n",
        "\n",
        "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0759 - acc: 0.9605 - val_loss: 0.0195 - val_acc: 0.9948\n",
        "Train on 60000 samples, validate on 10000 samples\n",
        "Epoch 1/20\n",
        "\n",
        "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
        "60000/60000 [==============================] - 15s 248us/step - loss: 0.0917 - acc: 0.9569 - val_loss: 0.0237 - val_acc: 0.9930\n",
        "Epoch 2/20\n",
        "\n",
        "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0875 - acc: 0.9557 - val_loss: 0.0231 - val_acc: 0.9939\n",
        "Epoch 3/20\n",
        "\n",
        "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0836 - acc: 0.9585 - val_loss: 0.0202 - val_acc: 0.9946\n",
        "Epoch 4/20\n",
        "\n",
        "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0809 - acc: 0.9582 - val_loss: 0.0210 - val_acc: 0.9940\n",
        "Epoch 5/20\n",
        "\n",
        "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0801 - acc: 0.9593 - val_loss: 0.0215 - val_acc: 0.9936\n",
        "Epoch 6/20\n",
        "\n",
        "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0787 - acc: 0.9590 - val_loss: 0.0216 - val_acc: 0.9943\n",
        "Epoch 7/20\n",
        "\n",
        "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0769 - acc: 0.9591 - val_loss: 0.0204 - val_acc: 0.9943\n",
        "Epoch 8/20\n",
        "\n",
        "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0760 - acc: 0.9600 - val_loss: 0.0214 - val_acc: 0.9949\n",
        "Epoch 9/20\n",
        "\n",
        "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0758 - acc: 0.9612 - val_loss: 0.0180 - val_acc: 0.9947\n",
        "Epoch 10/20\n",
        "\n",
        "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0768 - acc: 0.9598 - val_loss: 0.0174 - val_acc: 0.9952\n",
        "Epoch 11/20\n",
        "\n",
        "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0749 - acc: 0.9599 - val_loss: 0.0207 - val_acc: 0.9947\n",
        "Epoch 12/20\n",
        "\n",
        "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0759 - acc: 0.9602 - val_loss: 0.0186 - val_acc: 0.9948\n",
        "Epoch 13/20\n",
        "\n",
        "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0742 - acc: 0.9607 - val_loss: 0.0202 - val_acc: 0.9946\n",
        "Epoch 14/20\n",
        "\n",
        "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0739 - acc: 0.9595 - val_loss: 0.0177 - val_acc: 0.9946\n",
        "Epoch 15/20\n",
        "\n",
        "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
        "60000/60000 [==============================] - 6s 101us/step - loss: 0.0738 - acc: 0.9611 - val_loss: 0.0169 - val_acc: 0.9950\n",
        "Epoch 16/20\n",
        "\n",
        "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0714 - acc: 0.9623 - val_loss: 0.0176 - val_acc: 0.9952\n",
        "Epoch 17/20\n",
        "\n",
        "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0719 - acc: 0.9614 - val_loss: 0.0176 - val_acc: 0.9956\n",
        "Epoch 18/20\n",
        "\n",
        "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
        "60000/60000 [==============================] - 6s 103us/step - loss: 0.0742 - acc: 0.9599 - val_loss: 0.0178 - val_acc: 0.9948\n",
        "Epoch 19/20\n",
        "\n",
        "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0729 - acc: 0.9612 - val_loss: 0.0178 - val_acc: 0.9946\n",
        "Epoch 20/20\n",
        "\n",
        "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
        "60000/60000 [==============================] - 6s 102us/step - loss: 0.0723 - acc: 0.9605 - val_loss: 0.0172 - val_acc: 0.9951\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_zhiRNcG42C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "ead97a40-2c7d-4929-b316-19aba8a45c67"
      },
      "source": [
        " HISTORY"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pretty printing has been turned ON\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'10 20 8 kernels before max pooling': {'acc': [(0.9943, 0),\n",
              "   (0.9953, 1),\n",
              "   (0.996, 2),\n",
              "   (0.9956, 3)],\n",
              "  'log': '\\nTrain on 60000 samples, validate on 10000 samples\\nEpoch 1/20\\n\\nEpoch 00001: LearningRateScheduler setting learning rate to 0.003.\\n60000/60000 [==============================] - 15s 248us/step - loss: 0.5289 - acc: 0.8521 - val_loss: 0.0941 - val_acc: 0.9814\\nEpoch 2/20\\n\\nEpoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\\n60000/60000 [==============================] - 6s 100us/step - loss: 0.2575 - acc: 0.9224 - val_loss: 0.0698 - val_acc: 0.9858\\nEpoch 3/20\\n\\nEpoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.2017 - acc: 0.9400 - val_loss: 0.0435 - val_acc: 0.9898\\nEpoch 4/20\\n\\nEpoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.1731 - acc: 0.9458 - val_loss: 0.0400 - val_acc: 0.9905\\nEpoch 5/20\\n\\nEpoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.1540 - acc: 0.9483 - val_loss: 0.0325 - val_acc: 0.9914\\nEpoch 6/20\\n\\nEpoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\\n60000/60000 [==============================] - 6s 100us/step - loss: 0.1401 - acc: 0.9510 - val_loss: 0.0333 - val_acc: 0.9917\\nEpoch 7/20\\n\\nEpoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.1289 - acc: 0.9547 - val_loss: 0.0306 - val_acc: 0.9916\\nEpoch 8/20\\n\\nEpoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.1247 - acc: 0.9541 - val_loss: 0.0280 - val_acc: 0.9919\\nEpoch 9/20\\n\\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.1193 - acc: 0.9545 - val_loss: 0.0259 - val_acc: 0.9934\\nEpoch 10/20\\n\\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.1158 - acc: 0.9534 - val_loss: 0.0234 - val_acc: 0.9933\\nEpoch 11/20\\n\\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.1135 - acc: 0.9545 - val_loss: 0.0291 - val_acc: 0.9922\\nEpoch 12/20\\n\\nEpoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.1089 - acc: 0.9554 - val_loss: 0.0219 - val_acc: 0.9933\\nEpoch 13/20\\n\\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.1068 - acc: 0.9562 - val_loss: 0.0230 - val_acc: 0.9937\\nEpoch 14/20\\n\\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.1049 - acc: 0.9551 - val_loss: 0.0210 - val_acc: 0.9939\\nEpoch 15/20\\n\\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.1023 - acc: 0.9557 - val_loss: 0.0218 - val_acc: 0.9938\\nEpoch 16/20\\n\\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.1011 - acc: 0.9550 - val_loss: 0.0215 - val_acc: 0.9935\\nEpoch 17/20\\n\\nEpoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0977 - acc: 0.9572 - val_loss: 0.0196 - val_acc: 0.9942\\nEpoch 18/20\\n\\nEpoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0961 - acc: 0.9576 - val_loss: 0.0207 - val_acc: 0.9933\\nEpoch 19/20\\n\\nEpoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0958 - acc: 0.9564 - val_loss: 0.0204 - val_acc: 0.9943\\nEpoch 20/20\\n\\nEpoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0929 - acc: 0.9581 - val_loss: 0.0223 - val_acc: 0.9939\\nTrain on 60000 samples, validate on 10000 samples\\nEpoch 1/20\\n\\nEpoch 00001: LearningRateScheduler setting learning rate to 0.003.\\n60000/60000 [==============================] - 14s 232us/step - loss: 0.1151 - acc: 0.9524 - val_loss: 0.0430 - val_acc: 0.9878\\nEpoch 2/20\\n\\nEpoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.1077 - acc: 0.9533 - val_loss: 0.0295 - val_acc: 0.9920\\nEpoch 3/20\\n\\nEpoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.1010 - acc: 0.9537 - val_loss: 0.0242 - val_acc: 0.9932\\nEpoch 4/20\\n\\nEpoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0957 - acc: 0.9564 - val_loss: 0.0243 - val_acc: 0.9937\\nEpoch 5/20\\n\\nEpoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0956 - acc: 0.9553 - val_loss: 0.0224 - val_acc: 0.9930\\nEpoch 6/20\\n\\nEpoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0920 - acc: 0.9574 - val_loss: 0.0199 - val_acc: 0.9944\\nEpoch 7/20\\n\\nEpoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\\n60000/60000 [==============================] - 6s 100us/step - loss: 0.0896 - acc: 0.9577 - val_loss: 0.0202 - val_acc: 0.9947\\nEpoch 8/20\\n\\nEpoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0877 - acc: 0.9581 - val_loss: 0.0196 - val_acc: 0.9945\\nEpoch 9/20\\n\\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0870 - acc: 0.9589 - val_loss: 0.0210 - val_acc: 0.9939\\nEpoch 10/20\\n\\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0858 - acc: 0.9570 - val_loss: 0.0208 - val_acc: 0.9944\\nEpoch 11/20\\n\\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0837 - acc: 0.9594 - val_loss: 0.0204 - val_acc: 0.9939\\nEpoch 12/20\\n\\nEpoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\\n60000/60000 [==============================] - 6s 99us/step - loss: 0.0828 - acc: 0.9595 - val_loss: 0.0201 - val_acc: 0.9938\\nEpoch 13/20\\n\\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0844 - acc: 0.9582 - val_loss: 0.0194 - val_acc: 0.9953\\nEpoch 14/20\\n\\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0828 - acc: 0.9598 - val_loss: 0.0211 - val_acc: 0.9943\\nEpoch 15/20\\n\\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0806 - acc: 0.9604 - val_loss: 0.0200 - val_acc: 0.9944\\nEpoch 16/20\\n\\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\\n60000/60000 [==============================] - 6s 105us/step - loss: 0.0812 - acc: 0.9593 - val_loss: 0.0213 - val_acc: 0.9940\\nEpoch 17/20\\n\\nEpoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0805 - acc: 0.9587 - val_loss: 0.0193 - val_acc: 0.9945\\nEpoch 18/20\\n\\nEpoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\\n60000/60000 [==============================] - 6s 100us/step - loss: 0.0795 - acc: 0.9605 - val_loss: 0.0208 - val_acc: 0.9947\\nEpoch 19/20\\n\\nEpoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0801 - acc: 0.9605 - val_loss: 0.0211 - val_acc: 0.9942\\nEpoch 20/20\\n\\nEpoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0798 - acc: 0.9591 - val_loss: 0.0199 - val_acc: 0.9946\\nTrain on 60000 samples, validate on 10000 samples\\nEpoch 1/20\\n\\nEpoch 00001: LearningRateScheduler setting learning rate to 0.003.\\n60000/60000 [==============================] - 14s 238us/step - loss: 0.0961 - acc: 0.9564 - val_loss: 0.0309 - val_acc: 0.9910\\nEpoch 2/20\\n\\nEpoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0935 - acc: 0.9563 - val_loss: 0.0285 - val_acc: 0.9923\\nEpoch 3/20\\n\\nEpoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0894 - acc: 0.9564 - val_loss: 0.0277 - val_acc: 0.9931\\nEpoch 4/20\\n\\nEpoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0871 - acc: 0.9576 - val_loss: 0.0213 - val_acc: 0.9940\\nEpoch 5/20\\n\\nEpoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\\n60000/60000 [==============================] - 6s 100us/step - loss: 0.0845 - acc: 0.9586 - val_loss: 0.0192 - val_acc: 0.9949\\nEpoch 6/20\\n\\nEpoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0828 - acc: 0.9581 - val_loss: 0.0203 - val_acc: 0.9945\\nEpoch 7/20\\n\\nEpoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0832 - acc: 0.9581 - val_loss: 0.0186 - val_acc: 0.9952\\nEpoch 8/20\\n\\nEpoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0791 - acc: 0.9607 - val_loss: 0.0190 - val_acc: 0.9947\\nEpoch 9/20\\n\\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0814 - acc: 0.9581 - val_loss: 0.0183 - val_acc: 0.9949\\nEpoch 10/20\\n\\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0802 - acc: 0.9599 - val_loss: 0.0193 - val_acc: 0.9948\\nEpoch 11/20\\n\\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\\n60000/60000 [==============================] - 6s 106us/step - loss: 0.0790 - acc: 0.9599 - val_loss: 0.0175 - val_acc: 0.9951\\nEpoch 12/20\\n\\nEpoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0769 - acc: 0.9609 - val_loss: 0.0184 - val_acc: 0.9946\\nEpoch 13/20\\n\\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\\n60000/60000 [==============================] - 6s 105us/step - loss: 0.0772 - acc: 0.9603 - val_loss: 0.0174 - val_acc: 0.9953\\nEpoch 14/20\\n\\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0759 - acc: 0.9612 - val_loss: 0.0170 - val_acc: 0.9960\\nEpoch 15/20\\n\\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0765 - acc: 0.9606 - val_loss: 0.0188 - val_acc: 0.9953\\nEpoch 16/20\\n\\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0768 - acc: 0.9600 - val_loss: 0.0181 - val_acc: 0.9947\\nEpoch 17/20\\n\\nEpoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0740 - acc: 0.9608 - val_loss: 0.0182 - val_acc: 0.9953\\nEpoch 18/20\\n\\nEpoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0755 - acc: 0.9605 - val_loss: 0.0170 - val_acc: 0.9955\\nEpoch 19/20\\n\\nEpoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0757 - acc: 0.9600 - val_loss: 0.0168 - val_acc: 0.9953\\nEpoch 20/20\\n\\nEpoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0759 - acc: 0.9605 - val_loss: 0.0195 - val_acc: 0.9948\\nTrain on 60000 samples, validate on 10000 samples\\nEpoch 1/20\\n\\nEpoch 00001: LearningRateScheduler setting learning rate to 0.003.\\n60000/60000 [==============================] - 15s 248us/step - loss: 0.0917 - acc: 0.9569 - val_loss: 0.0237 - val_acc: 0.9930\\nEpoch 2/20\\n\\nEpoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0875 - acc: 0.9557 - val_loss: 0.0231 - val_acc: 0.9939\\nEpoch 3/20\\n\\nEpoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0836 - acc: 0.9585 - val_loss: 0.0202 - val_acc: 0.9946\\nEpoch 4/20\\n\\nEpoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0809 - acc: 0.9582 - val_loss: 0.0210 - val_acc: 0.9940\\nEpoch 5/20\\n\\nEpoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0801 - acc: 0.9593 - val_loss: 0.0215 - val_acc: 0.9936\\nEpoch 6/20\\n\\nEpoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0787 - acc: 0.9590 - val_loss: 0.0216 - val_acc: 0.9943\\nEpoch 7/20\\n\\nEpoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0769 - acc: 0.9591 - val_loss: 0.0204 - val_acc: 0.9943\\nEpoch 8/20\\n\\nEpoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0760 - acc: 0.9600 - val_loss: 0.0214 - val_acc: 0.9949\\nEpoch 9/20\\n\\nEpoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0758 - acc: 0.9612 - val_loss: 0.0180 - val_acc: 0.9947\\nEpoch 10/20\\n\\nEpoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0768 - acc: 0.9598 - val_loss: 0.0174 - val_acc: 0.9952\\nEpoch 11/20\\n\\nEpoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0749 - acc: 0.9599 - val_loss: 0.0207 - val_acc: 0.9947\\nEpoch 12/20\\n\\nEpoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0759 - acc: 0.9602 - val_loss: 0.0186 - val_acc: 0.9948\\nEpoch 13/20\\n\\nEpoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0742 - acc: 0.9607 - val_loss: 0.0202 - val_acc: 0.9946\\nEpoch 14/20\\n\\nEpoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0739 - acc: 0.9595 - val_loss: 0.0177 - val_acc: 0.9946\\nEpoch 15/20\\n\\nEpoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0738 - acc: 0.9611 - val_loss: 0.0169 - val_acc: 0.9950\\nEpoch 16/20\\n\\nEpoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0714 - acc: 0.9623 - val_loss: 0.0176 - val_acc: 0.9952\\nEpoch 17/20\\n\\nEpoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0719 - acc: 0.9614 - val_loss: 0.0176 - val_acc: 0.9956\\nEpoch 18/20\\n\\nEpoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\\n60000/60000 [==============================] - 6s 103us/step - loss: 0.0742 - acc: 0.9599 - val_loss: 0.0178 - val_acc: 0.9948\\nEpoch 19/20\\n\\nEpoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0729 - acc: 0.9612 - val_loss: 0.0178 - val_acc: 0.9946\\nEpoch 20/20\\n\\nEpoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\\n60000/60000 [==============================] - 6s 102us/step - loss: 0.0723 - acc: 0.9605 - val_loss: 0.0172 - val_acc: 0.9951\\n',\n",
              "  'non_trainable_params': 208,\n",
              "  'scores': [[0.022299894432898145, 0.9939],\n",
              "   [0.019910878463128757, 0.9946],\n",
              "   [0.019506690664345298, 0.9948],\n",
              "   [0.017212786722480996, 0.9951]],\n",
              "  'trainable_params': 12882},\n",
              " 'original without bias': {'acc': [(0.9959, 0),\n",
              "   (0.9956, 1),\n",
              "   (0.9956, 2),\n",
              "   (0.9955, 3)],\n",
              "  'non_trainable_params': 244,\n",
              "  'scores': [[0.01808474469958569, 0.9954],\n",
              "   [0.019497062973319226, 0.9952],\n",
              "   [0.01994227597291588, 0.9949],\n",
              "   [0.021506337354881406, 0.9952]],\n",
              "  'trainable_params': 16228}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    }
  ]
}